namespace OpenAI;
using TypeSpec.OpenAPI;

alias CHAT_COMPLETION_MODELS =
  | "gpt4"
  | "gpt-4-0314"
  | "gpt-4-0613"
  | "gpt-4-32k"
  | "gpt-4-32k-0314"
  | "gpt-4-32k-0613"
  | "gpt-3.5-turbo"
  | "gpt-3.5-turbo-16k"
  | "gpt-3.5-turbo-0301"
  | "gpt-3.5-turbo-0613"
  | "gpt-3.5-turbo-16k-0613";

alias COMPLETION_MODELS =
  | "babbage-002"
  | "davinci-002"
  | "text-davinci-003"
  | "text-davinci-002"
  | "text-davinci-001"
  | "code-davinci-002"
  | "text-curie-001"
  | "text-babbage-001"
  | "text-ada-001";

alias SharedCompletionProperties = {
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
   * more random, while lower values like 0.2 will make it more focused and deterministic.
   *
   * We generally recommend altering this or `top_p` but not both.
   */
  temperature?: Temperature | null = 1;

  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers
   * the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
   * the top 10% probability mass are considered.
   *
   * We generally recommend altering this or `temperature` but not both.
   */
  top_p?: TopP | null = 1;

  /**
   * How many completions to generate for each prompt.
   * **Note:** Because this parameter generates many completions, it can quickly consume your token
   * quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
   */
  n?: N | null = 1;

  /**
   * The maximum number of [tokens](/tokenizer) to generate in the completion.
   *
   * The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
   * for counting tokens.
   */
  max_tokens?: MaxTokens | null = 16;

  // todo: consider inlining when https://github.com/microsoft/typespec/issues/2356 is resolved
  // https://github.com/microsoft/typespec/issues/2355
  /** Up to 4 sequences where the API will stop generating further tokens. */
  stop?: Stop = null;

  // needs default
  // https://github.com/microsoft/typespec/issues/1646
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
   * in the text so far, increasing the model's likelihood to talk about new topics.
   *
   * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
   */
  presence_penalty?: Penalty | null;

  // needs default
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
   * frequency in the text so far, decreasing the model's likelihood to repeat the same line
   * verbatim.
   *
   * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)
   */
  frequency_penalty?: Penalty | null;

  // needs default of null
  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an
   * associated bias value from -100 to 100. Mathematically, the bias is added to the logits
   * generated by the model prior to sampling. The exact effect will vary per model, but values
   * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
   * should result in a ban or exclusive selection of the relevant token.
   */
  @extension("x-oaiTypeLabel", "map")
  logit_bias?: Record<safeint> | null;

  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor and detect
   * abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
   */
  user?: User;

  /**
   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only
   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
   * as they become available, with the stream terminated by a `data: [DONE]` message.
   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).
   */
  stream?: boolean | null = true;
};

@oneOf
union Stop {
  string,
  StopSequences,
  null,
}

@minValue(-2)
@maxValue(2)
scalar Penalty extends float64;

@minItems(1)
@maxItems(4)
model StopSequences is string[];

@minValue(0)
@maxValue(2)
scalar Temperature extends float64;

@minValue(0)
@maxValue(1)
scalar TopP extends float64;

@minValue(1)
@maxValue(128)
scalar N extends safeint;

@minValue(0)
scalar MaxTokens extends safeint;

model CreateChatCompletionRequest {
  /**
   * ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility)
   * table for details on which models work with the Chat API.
   */
  @extension("x-oaiTypeLabel", "string")
  `model`: string | CHAT_COMPLETION_MODELS;

  /**
   * A list of messages comprising the conversation so far.
   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
   */
  @minItems(1)
  messages: ChatCompletionRequestMessage[];

  /** A list of functions the model may generate JSON inputs for. */
  @minItems(1)
  @maxItems(128)
  functions?: ChatCompletionFunctions[];

  /**
   * Controls how the model responds to function calls. `none` means the model does not call a
   * function, and responds to the end-user. `auto` means the model can pick between an end-user or
   * calling a function.  Specifying a particular function via `{\"name":\ \"my_function\"}` forces the
   * model to call that function. `none` is the default when no functions are present. `auto` is the
   * default if functions are present.
   */
  function_call?: "none" | "auto" | ChatCompletionFunctionCallOption;

  ...SharedCompletionProperties;
}

model ChatCompletionFunctionCallOption {
  /** The name of the function to call. */
  name: string;
}

model ChatCompletionFunctions {
  /**
   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
   * dashes, with a maximum length of 64.
   */
  name: string;

  /**
   * A description of what the function does, used by the model to choose when and how to call the
   * function.
   */
  description?: string;

  /**
   * The parameters the functions accepts, described as a JSON Schema object. See the
   * [guide](/docs/guides/gpt/function-calling) for examples, and the
   * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation
   * about the format.\n\nTo describe a function that accepts no parameters, provide the value
   * `{\"type\": \"object\", \"properties\": {}}`.
   */
  parameters: ChatCompletionFunctionParameters;
}

model ChatCompletionFunctionParameters is Record<unknown>;

model ChatCompletionRequestMessage {
  /** The role of the messages author. One of `system`, `user`, `assistant`, or `function`. */
  role: "system" | "user" | "assistant" | "function";

  /**
   * The contents of the message. `content` is required for all messages, and may be null for
   * assistant messages with function calls.
   */
  content: string | null;

  // TODO: the constraints are not specified in the API
  /**
   * The name of the author of this message. `name` is required if role is `function`, and it
   * should be the name of the function whose response is in the `content`. May contain a-z,
   * A-Z, 0-9, and underscores, with a maximum length of 64 characters.
   */
  name?: string;

  /** The name and arguments of a function that should be called, as generated by the model. */
  function_call?: {
    /** The name of the function to call. */
    name: string;

    /**
     * The arguments to call the function with, as generated by the model in JSON format. Note that
     * the model does not always generate valid JSON, and may hallucinate parameters not defined by
     * your function schema. Validate the arguments in your code before calling your function.
     */
    arguments: string;
  };
}

/** Represents a chat completion response returned by model, based on the provided input. */
// TODO: Fill in example here.
@extension(
  "x-oaiMeta",
  {
    name: "The chat completion object",
    group: "chat",
    example: "",
  }
)
model CreateChatCompletionResponse {
  /** A unique identifier for the chat completion. */
  id: string;

  /** The object type, which is always `chat.completion`. */
  object: string;

  /** The Unix timestamp (in seconds) of when the chat completion was created. */
  @encode("unixTimestamp", int32)
  created: utcDateTime;

  /** The model used for the chat completion. */
  `model`: string;

  /** A list of chat completion choices. Can be more than one if `n` is greater than 1. */
  choices: {
    /** The index of the choice in the list of choices. */
    index: safeint;

    message: ChatCompletionResponseMessage;

    /**
     * The reason the model stopped generating tokens. This will be `stop` if the model hit a
     * natural stop point or a provided stop sequence, `length` if the maximum number of tokens
     * specified in the request was reached, `content_filter` if the content was omitted due to
     * a flag from our content filters, or `function_call` if the model called a function.
     */
    finish_reason: "stop" | "length" | "function_call" | "content_filter";
  }[];

  usage?: CompletionUsage;
}

/** Usage statistics for the completion request. */
model CompletionUsage {
  /** Number of tokens in the prompt. */
  prompt_tokens: safeint;

  /** Number of tokens in the generated completion */
  completion_tokens: safeint;

  /** Total number of tokens used in the request (prompt + completion). */
  total_tokens: safeint;
}

model ChatCompletionResponseMessage {
  /** The role of the author of this message. */
  role: "system" | "user" | "assistant" | "function";

  /** The contents of the message. */
  content: string | null;

  /** The name and arguments of a function that should be called, as generated by the model. */
  function_call?: {
    /** The name of the function to call. */
    name: string;

    /**
     * The arguments to call the function with, as generated by the model in JSON format. Note that
     * the model does not always generate valid JSON, and may hallucinate parameters not defined by
     * your function schema. Validate the arguments in your code before calling your function.
     */
    arguments: string;
  };
}

model CreateCompletionRequest {
  /**
   * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to
   * see all of your available models, or see our [Model overview](/docs/models/overview) for
   * descriptions of them.
   */
  @extension("x-oaiTypeLabel", "string")
  `model`: string | COMPLETION_MODELS;

  /**
   * The prompt(s) to generate completions for, encoded as a string, array of strings, array of
   * tokens, or array of token arrays.
   *
   * Note that <|endoftext|> is the document separator that the model sees during training, so if a
   * prompt is not specified the model will generate as if from the beginning of a new document.
   */
  // TODO: consider inlining when https://github.com/microsoft/typespec/issues/2356 fixed
  prompt: Prompt = "<|endoftext|>";

  /** The suffix that comes after a completion of inserted text. */
  suffix?: string | null = null;

  ...SharedCompletionProperties;

  /**
   * Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens.
   * For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The
   * API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1`
   * elements in the response.
   *
   * The maximum value for `logprobs` is 5.
   */
  logprobs?: safeint | null = null;

  /** Echo back the prompt in addition to the completion */
  echo?: boolean | null = false;

  /**
   * Generates `best_of` completions server-side and returns the "best" (the one with the highest
   * log probability per token). Results cannot be streamed.
   *
   * When used with `n`, `best_of` controls the number of candidate completions and `n` specifies
   * how many to return â€“ `best_of` must be greater than `n`.
   *
   * **Note:** Because this parameter generates many completions, it can quickly consume your token
   * quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
   */
  best_of?: safeint | null = 1;
}

@oneOf
union Prompt {
  string,
  string[],
  TokenArray,
  TokenArrayArray,
  null,
}
/**
 * Represents a completion response from the API. Note: both the streamed and non-streamed response
 * objects share the same shape (unlike the chat endpoint).
 */
@extension(
  "x-oaiMeta",
  {
    name: "The  completion object",
    legacy: true,
    example: "", // fill in
  }
)
model CreateCompletionResponse {
  /** A unique identifier for the completion. */
  id: string;

  /** The object type, which is always `text_completion`. */
  object: string;

  /** The Unix timestamp (in seconds) of when the completion was created. */
  @encode("unixTimestamp", int32)
  created: utcDateTime;

  /** The model used for the completion. */
  `model`: string;

  /** The list of completion choices the model generated for the input. */
  choices: {
    index: safeint;
    text: string;
    logprobs: null | {
      tokens: string[];
      token_logprobs: float64[];
      top_logprobs: Record<safeint>[];
      text_offset: safeint[];
    };

    /**
     * The reason the model stopped generating tokens. This will be `stop` if the model hit a
     * natural stop point or a provided stop sequence, or `content_filter` if content was omitted
     * due to a flag from our content filters, `length` if the maximum number of tokens specified
     * in the request was reached, or `content_filter` if content was omitted due to a flag from our
     * content filters.
     */
    finish_reason: "stop" | "length" | "content_filter";
  }[];

  usage?: CompletionUsage;
}
